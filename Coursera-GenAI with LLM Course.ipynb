{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f4dd8024",
   "metadata": {},
   "source": [
    "- In this notebook, we will get a high level overview of major topics in LLM field.\n",
    "- In future notebooks, we will deepdive into each topics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4be6d39f",
   "metadata": {},
   "source": [
    "- Text we pass to model is called prompt\n",
    "- Space available for the prompt (prompt length) is context window.\n",
    "- output of a model is called completion.\n",
    "- Act of using a model to generate some output is called inference."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0452ff19",
   "metadata": {},
   "source": [
    "### Transformer Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff999ef7",
   "metadata": {},
   "source": [
    "- Transformers changed the Generative Ai field. \n",
    "- Before this architecture, previous architectures like rnn were not good in generating and also it is not possible to scale them to understand the context. \n",
    "- Transformer came and changed everything. It is scalable, can process paralelly, gives attention to meaning of input words it is processing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8876624",
   "metadata": {},
   "source": [
    "- First we need to use tokenizer to convert text into numbers, this must be same tokenizer used while pretraining.\n",
    "- Next passes to embedding layer.Each token id is matched into multi dimensional vector.\n",
    "- You also add positional encoding to preserve the word order.\n",
    "- Next we will pass this result into self attention layer, Here model analyzes the relation btw tokens and input sequence to get capture contexual dependencies btw words.\n",
    "- But this doesnt happend only once.Transformer has multi headed self attention . This means multiple sets of attention heads are learnt in parallel independently of each other.This varies from 12-100.Each self attention head will learn different aspects of language.\n",
    "- Now that attention has been applied to input data,this output is processed through feed forward network.\n",
    "- Output of this layer is vector of logits proportional to probability score for each token in tokenizer dictionary(means list of full vocab words)\n",
    "- We pass this logits to softmax layer where they are normalized into probability score of each word in vocabulary.\n",
    "- Now from this list we will get token output based on highest probability score word. But this can be varied with some parameters like temp etc\n",
    "- Now this is just basic overview of each layers. Let us see how three different variations of transformer architecture works with prompt input given.\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "342933b3",
   "metadata": {},
   "source": [
    "- 1. Encoder-Only Architecture\n",
    "\n",
    "Example Model: BERT\n",
    "Typical Task: Text classification, sentiment analysis, named entity recognition (NER)\n",
    "Pipeline for Encoder-Only Model:\n",
    "In an encoder-only model like BERT, the model is focused on understanding and processing input to extract meaningful representations (contextual embeddings) for downstream tasks, such as classification or question answering.\n",
    "\n",
    "Step 1: Tokenization\n",
    "\n",
    "You provide a prompt: “The Eiffel Tower is in Paris.”\n",
    "The tokenizer converts the prompt into token IDs (e.g., [101, 1000, 2325, 2884, 2003, 1999, 3000, 102]).\n",
    "Step 2: Embedding Layer\n",
    "\n",
    "These token IDs are passed through an embedding layer to convert them into continuous vector representations, with positional encodings added to preserve word order.\n",
    "\n",
    "Step 3: Self-Attention\n",
    "\n",
    "The encoder uses self-attention mechanisms to capture the relationships between tokens and their context. For example, it might understand that \"Eiffel Tower\" is a location, and \"Paris\" is related to it.\n",
    "\n",
    "Step 4: Multi-Headed Self-Attention\n",
    "\n",
    "The model uses multiple attention heads to learn different relationships in parallel, capturing various aspects of the input like syntax, semantics, and dependencies.\n",
    "\n",
    "Step 5: Feed-Forward and Final Layer\n",
    "\n",
    "The output of the self-attention layers is passed through a feed-forward neural network. The final output is a set of logits or a vector of token representations.\n",
    "\n",
    "Step 6: Task-Specific Head\n",
    "\n",
    "If the task is classification (like sentiment analysis), the representation of the special [CLS] token at the start of the sequence is passed through a classification head. The model makes predictions based on this.\n",
    "\n",
    "Use Case:\n",
    "\n",
    "For a classification task, the output might be a label, like \"positive\" or \"negative\", depending on the input text.\n",
    "\n",
    "- 2. Encoder-Decoder Architecture\n",
    "\n",
    "Example Model: T5 (Text-to-Text Transfer Transformer), BART\n",
    "\n",
    "Typical Task: Text generation, machine translation, summarization\n",
    "\n",
    "Pipeline for Encoder-Decoder Model:\n",
    "\n",
    "Encoder-decoder models are designed for tasks where both input and output sequences are involved. For example, machine translation or summarization.\n",
    "\n",
    "Step 1: Tokenization of Input (Prompt)\n",
    "\n",
    "Suppose the task is summarization: The prompt might be a long article.\n",
    "Example prompt: “The Eiffel Tower is a wrought-iron lattice tower on the Champ de Mars in Paris...”\n",
    "The tokenizer converts the input into token IDs and embeddings, just like in the encoder-only architecture.\n",
    "\n",
    "Step 2: Encoding\n",
    "\n",
    "The encoder processes the input sequence to learn a meaningful representation of the text.\n",
    "Self-attention and feed-forward layers are applied, capturing the relationships between words in the input sequence.\n",
    "\n",
    "Step 3: Decoder Input\n",
    "\n",
    "The decoder is then activated, but it requires both the encoded input and a partial output sequence.\n",
    "At the start of generation, the output sequence is usually an initial token like <start>.\n",
    "    \n",
    "Step 4: Cross-Attention in Decoder\n",
    "\n",
    "The decoder has access to both its own previous outputs (via self-attention) and the representations from the encoder via cross-attention.\n",
    "The decoder generates the next token based on both the context from the encoder and its previous output tokens.\n",
    "    \n",
    "Step 5: Token-by-Token Generation\n",
    "\n",
    "The decoder generates the output sequence token by token. For example, if the task is summarization, the decoder would generate: “The Eiffel Tower is located in Paris.”\n",
    "    \n",
    "Use Case:\n",
    "\n",
    "For machine translation, the input could be an English sentence (\"The Eiffel Tower is in Paris\"), and the model would output the French translation (\"La tour Eiffel est à Paris\").\n",
    "    \n",
    "- 3. Decoder-Only Architecture\n",
    "    \n",
    "Example Model: GPT-4, GPT-3, LLaMA\n",
    "    \n",
    "Typical Task: Text generation, dialogue systems, open-ended generation\n",
    "    \n",
    "Pipeline for Decoder-Only Model:\n",
    "    \n",
    "Decoder-only models are designed for generating text based on a prompt. They only have a decoder and generate text autoregressively (one token at a time).\n",
    "\n",
    "Step 1: Tokenization of Prompt\n",
    "\n",
    "Let’s say your prompt is: “Write a story about a dog and a cat.”\n",
    "The tokenizer converts the prompt into token IDs (e.g., [1567, 23, 99, 452, 345]).\n",
    "    \n",
    "Step 2: Embedding Layer\n",
    "\n",
    "These token IDs are passed through an embedding layer, which transforms them into continuous vector representations.\n",
    "    \n",
    "Step 3: Decoder Self-Attention\n",
    "\n",
    "The decoder processes the embeddings using self-attention to learn relationships between the tokens in the prompt. For a decoder-only model, the prompt is essential because it defines the context for generating the next token.\n",
    "    \n",
    "Step 4: Autoregressive Token Generation\n",
    "\n",
    "The model predicts the next token based on the prompt, using its knowledge of language patterns. After predicting the next token, that token is added to the prompt, and the model predicts the next one in sequence.\n",
    "    \n",
    "Step 5: Repeat for Full Generation\n",
    "\n",
    "The model continues predicting the next token one by one until it reaches the end of the sequence. For example, after processing the prompt, the model might generate a sentence like: “The dog and the cat became best friends and lived together.”\n",
    "    \n",
    "Use Case:\n",
    "\n",
    "This pipeline is perfect for story generation, question answering, or dialogue systems. You can pass in a prompt, and the model will generate text based on that input.\n",
    "    \n",
    "Summary of Pipelines:\n",
    "    \n",
    "Encoder-Only (BERT):\n",
    "\n",
    "Focus: Understanding and representation.\n",
    "    \n",
    "Task: Classification, token classification, NER.\n",
    "    \n",
    "Prompt: Passed to encoder, results in output representations or logits for tasks like classification.\n",
    "    \n",
    "Encoder-Decoder (T5, BART):\n",
    "\n",
    "Focus: Sequence-to-sequence tasks.\n",
    "    \n",
    "Task: Translation, summarization.\n",
    "    \n",
    "Prompt: Passed to encoder, output sequence is generated by the decoder with cross-attention on the encoded input.\n",
    "\n",
    "Decoder-Only (GPT-4, LLaMA):\n",
    "\n",
    "Focus: Text generation.\n",
    "Task: Story generation, dialogue, open-ended tasks.\n",
    "Prompt: Passed to decoder, which generates the next token autoregressively based on the context."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed2ef1f8",
   "metadata": {},
   "source": [
    "# Generative AI Project LifeCycle:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60831bab",
   "metadata": {},
   "source": [
    "1. Define the problem.\n",
    "2. Chose the model\n",
    "3. Adapt and align the model\n",
    "4. Evaluate\n",
    "5. Optimize and deploy model for inference\n",
    "6. Build llm powered applications"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e8e249e",
   "metadata": {},
   "source": [
    "### 3 Ways to Adapt and Align the model:\n",
    "1. Prompting : \n",
    "Example is Few shot incontext learning\n",
    "2. Fine tuning : Example is lora\n",
    "3. Align with human feedback:Example is RLHF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3135dbfb",
   "metadata": {},
   "source": [
    "### Prompting\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "167536de",
   "metadata": {},
   "source": [
    "- We can engineer our prompt for the model by Providing examples inside the context window is called in-context learning. \n",
    "- It can be zero , one or few shot inference, (one shot means one example input and output in prompt).\n",
    "- If model doesnt behave well till may be 5 shots, then fine tuning or going for higher parameter model or better model is the option."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "769475ca",
   "metadata": {},
   "source": [
    "### Generative Configurations:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd18248a",
   "metadata": {},
   "source": [
    "\n",
    "- These are the configuration parameters that we can use to influence the final decision of model about next word generation.\n",
    "- These are only invoked during inference time and are not the model parameters learned during training time i.e. trainable parameters \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8104884b",
   "metadata": {},
   "source": [
    "1. max new tokens: to limit the  number of tokens model will generate, if end of sequence token reaches before this limit , then ofcourse model wont predict further. if not this will the the limit.\n",
    "\n",
    "- Output from softmax layer is a probability distribution of entire dictionary of words that model uses. Consider this is the dictionary \n",
    "\n",
    "    {mango:0.3,apple:0.2, banana:0.05, hello:0.02, bus:0.01,....}\n",
    "    \n",
    "    By default most llm selects top probability word as next word. i.e. greedy approach or Greedy Sampling.\n",
    "    \n",
    "    But this is ok for short generation but if prediction is long then susseptible to repeated words,\n",
    "    \n",
    "    So it is better to have some randomness or creative predictions. i.e Random Sampling\n",
    "    \n",
    "    But if we just do it randomly across all words, then some irrelevant word will be predicted.\n",
    "    \n",
    "    So we can limit the dictionary of words for prediction and can chose our random word from that. \n",
    "    \n",
    "    So Top-p and Top-k are such parameters to select an output using random-weighted sampling.\n",
    "   \n",
    "2. Top-k : This is one way we can limit random sampling and make output sensible. So in this we specify a number, and then we limit our dictionary of words into that number , then we randomly sample from that limited dictionary.\n",
    "\n",
    "      For example , if we have top_k=3, then out of top 3 probabilistic words i.e mango, apple and banana, one will be randomly selected as next token.\n",
    "\n",
    "3. Top-p : Here also instead of number, we fix a probability, i.e for example like 0.5, so we select top  probability words whose sum of probability is within 0.5, and randomly sample from those. \n",
    "    That means sum of probability of mango and apple is 0.5, so only these words will fit, so one from these two will be randomly selected as next token. cumulative probability<=p\n",
    "    \n",
    "4. Temperature: This parameter is also used to control the randomness of model output. higher the value , higher the randomness, and vice versa.\n",
    "\n",
    "    This parameter influence the shape of probability distribution the model calculates for predicting next      token.\n",
    "    \n",
    "    Temperature value is the scaling factor that is supplied in the softmax layer of the model.\n",
    "    \n",
    "    Default value will be 1 that means distribution is scaled to 1. if we decrease temp, <1 means, distribution will be small, that means probability will be distributed among smaller number of words for the model to chose from, from these words model will chose from random sampling.\n",
    "    If temp >1, probability distribution will be broader and less peaked, so more words to chose from for the next word. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc3628d8",
   "metadata": {},
   "source": [
    "# Fine Tuning LLM : "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15520534",
   "metadata": {},
   "source": [
    "Fine tuning can be done to imporve the performance of the existing model for specific use case"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f3cdf1a",
   "metadata": {},
   "source": [
    "- One particular most common strategy used in finetuning is instruction fine tuning.\n",
    "- In this we give prompt:completion pair i.e labelled dataset for the model\n",
    "- We can generate this instruction labelled dataset using several prompt templates available.\n",
    "- Fine tuning is a supervised process where as pretraining is a semi supervised one.\n",
    "- Instruction fine tuning where all the model weights are updated is called full fine tuning.\n",
    "- We split the training data into train ,test and and validation set., then after training we do evaluatation\n",
    "- Training data is passed to model and we get output, as we know output of the model is probability distribution across tokens. we compare this with our label and use cross entropy function to calculate loss and to update weights using back propogation. So this will happen across multiple batches of training data and epochs as usual.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "417b9917",
   "metadata": {},
   "source": [
    "- So these fine tuning can be done on single tasks or for multiple tasks.\n",
    "- If you are sure you need the model for only one task say sentiment analysis, then take around 1000 sized dataset and train it.It doesnt require much data.\n",
    "- But the issue with this is it may lead to catastropic forgetting , that means fine tuned model may forget what it has learnt in pretraining and cannot perform other tasks that it was trained on. So in our model will give good results on sentiment analysis but it may not be able to perform NER for example.\n",
    "- To avoid catastropic forgetting , if that is problem to you, then we can go with fine tuning on multiple tasks or also PEFT.\n",
    "- In Multi task fine tuning , we provide mixed dataset {prompt:completion} for multiple tasks, so model will be good at multiple tasks , but we need lots of dataset , examples for these are generally the instruct models got from base models, they do in same process.\n",
    "- Now after fine tuning, we need to evaluate the training/model as we do in typical ml , but in these outputs are not deterministic, so we cant use metrics like accuracy directly. \n",
    "- For specific tasks, there are certain evaluation methods like for example, for summarization:Rogue score,\n",
    "For translation: Bleu score. \n",
    "- These methods are not suitable for all tasks, and also not best either, so check for new evaluation metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9440d27",
   "metadata": {},
   "source": [
    "- General Purpose models can be evaluated using Benchmarks like Glue,Super Glue, HELM,MMLU\n",
    "- Now these are used to measure performance of llms on wide variety of things like its intelligence, math solving ability, coding ablility, world knowledge, common sense,reasoning etc. Models will be tested on these different things based on dataset in these benchmarks.\n",
    "- These are the things that we see in model release page also, where they will list the score of model for these benchmarks.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be280aa5",
   "metadata": {},
   "source": [
    "### PEFT:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6319b3c",
   "metadata": {},
   "source": [
    "- Full fine tuning often required lot of hardware to store trainable weights allong with optimizer states,gradients,forward activations,temporary memory. \n",
    "- In PEFT:Parameter Efficient Fine tuning, as the name suggests this is the efficient way of fine tuning a llm, with not much of tradeoffs in efficiency compared to full finetuning.Often this can be performed on single GPU also.\n",
    "- In this we only finetune a subset of existing parameters or add new layer of parameters and finetune them. That means either all or most of the weights are kept forzen based on the type of peft method we are doing.\n",
    "- PEFT is less prone to catastropic forgetting since all/many parameters are left unchanged unlike full fine tuning,\n",
    "- PEFT weights can be trained for each task, and therefore swapped based on requirement during inference.this leads to adoptation of model for multiple tasks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03afdde6",
   "metadata": {},
   "source": [
    "- There are several methods that can be used for PEFT based on different tradeoffs,\n",
    "    1. Memory Efficiency\n",
    "    2. Parameter Efficiency\n",
    "    3. Training Speed\n",
    "    4. Inference Costs\n",
    "    5. Model Performance.\n",
    "   \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b474bb3",
   "metadata": {},
   "source": [
    "- Three main classes of Peft methods:\n",
    "    1. Selective Methods: Select subset of initial model parameters for finetune. We can identify these trainable parameters like either select a parameter type or specific layer or certain component based on requirement.\n",
    "    2. Reparameterize Methods: This also works with initial model parameters but it reduces the the parameters to train by creating new low rank representation of origin network weights. Example for this is LORA.\n",
    "    3. Additive MEthods: In these methods, Pretraine model weights will remain same, but we add addition trainable parameters/layers. In this there are two main approaches.\n",
    "        - Adapters add new trainable params/layers to the architecure of the model inside encode/decoder components after feedforward layer/attention layer. \n",
    "        - Other method is soft prompts methods ,one example is Prompt tuning.\n",
    "        This keeps architecture fixed but focus of manipulating inputs.This can be done by adding trainable parameters to prompt embeddings or by retraining embedding weights."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9a40bae",
   "metadata": {},
   "source": [
    "#### LORA:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fad7889c",
   "metadata": {},
   "source": [
    "Low Rank Adaptation of Large language model is a peft technique that falls in reparametrized category.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33bead7a",
   "metadata": {},
   "source": [
    "- Training Steps:\n",
    "    1. Freeze most of the model weights\n",
    "    2. Inject two rank decomposition matrices A and B, multiply them. B*A to get the same dimension of the weights they are moifying.\n",
    "    3. Train the weights of smaller matrices using supervised training like how we saw before.\n",
    "-  For Inference, Add these two origin weights and update them in the original matrix. updated weights+B*A\n",
    "- \n",
    "We can apply lora just on self attention component and it wil be good enough based on research, but in general we can also apply on other components like feed forward layer etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1450d748",
   "metadata": {},
   "source": [
    " we can take rank r between 4 to 32, still no proper rules actually, but based on best approaches, this range will be fine since less r means less trainable params and vice versa. and also more r doesnt guarantee best performance. So this range will be good,"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b209e571",
   "metadata": {},
   "source": [
    "- During inference, we can interchange this pair of matrices we trained based on tasks. For different tasks, you can have different trained rank decomposition matrices, during inference , take that respective pair, multiple them, add and update them in the origin weights and then make inference on that particular task."
   ]
  },
  {
   "cell_type": "raw",
   "id": "beb4e5be",
   "metadata": {},
   "source": [
    "Example :\n",
    "Model weight:512*64\n",
    "For full training, total trainable parameters is 512*64=32768\n",
    "Now if we use lora, consider rank r =8\n",
    "This means matrix A will have dimension 8*64=512 trainble parameters\n",
    "B will have dimenstion 8*512=4096 trainable params\n",
    "total = 4608 trainable params. makes 86% less trainable params that full finetuning."
   ]
  },
  {
   "cell_type": "raw",
   "id": "c77a05a9",
   "metadata": {},
   "source": [
    "Rank of a matrix: number of linearly independent rows/columns \n",
    "if we have a matrix like \n",
    "\n",
    "1 2 5 2 4\n",
    "3 3 9 6 9\n",
    "2 3 8 5 7\n",
    "4 1 6 5 9\n",
    "now in this matrix, we can say c1 is independent since this is first column   \n",
    "c2 is also linearly independent column since this doesnt depend on previous column\n",
    "c3= 1 * c1 + 2 * c2\n",
    "c4= 1 * c1 + 1 * c2\n",
    "c5= 2 * c1 + 1 * c2\n",
    "\n",
    "c1=1*c1+0*c2\n",
    "c2=0*c1+1*c2\n",
    "So that means only 2 independent columns, so rank is 2.\n",
    "this column can be decomposed into\n",
    "[ 1 2 ]    \n",
    "[ 3 3 ]   *   [1 0 1 1 2]\n",
    "[ 2 3 ]       [0 1 2 1 1]\n",
    "[ 4 1 ]\n",
    "\n",
    "So basically the product of these two matrix is our original matrix\n",
    "4 X 2   *   2 X 5  =  4 X 5\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef376c94",
   "metadata": {},
   "source": [
    "Adapters:\n",
    "Let us take BERT large model: Bidirectional Encoder only Representation of Transformers\n",
    "This has 24 transformer blocks.It has around 354Million parameters\n",
    "\n",
    "input-> T1 -> T2 .. -> T24  -> output\n",
    "\n",
    "Each Transformers block will have ulti head attention, feed forward layer , layer normalization etc.\n",
    "\n",
    "Now we cant finetune all 345M params.\n",
    "What i can do is after multi head attention and feed forward layer, i can add an Adapter in each transformer layer.\n",
    "\n",
    "Adapter helps adapt to a finetuned task.\n",
    "Adapters are small, trainable modules crafted to be both lightweight and modular, seamlessly integrating at various points within an LLM architecture.By selectively fine-tuning these specific modules rather than the entire model, adapters facilitate the customization of pre-trained models for distinct tasks, requiring only a minimal increase in parameters.\n",
    "So coming to peft, we add an adapter and we update the weights of only that adapter while freezing all weights of the model.\n",
    "Now instead of doing adapter weight finetunig, we can "
   ]
  },
  {
   "cell_type": "raw",
   "id": "4ccdf585",
   "metadata": {},
   "source": [
    "Now coming to LORA, We add low rank adapters.\n",
    "Instead of using just an adapter, we can use low rank decomposed matrices which will still decrease the trainable parameters\n",
    "compared to just using a adapter.\n",
    "This lora will take two low rank matrices whose product will be same as that of the layer where we are putting it.\n",
    "Basically this is also adding adapter, but just the low rank matrix instead of one.\n",
    "Lora used SVD (singular value decomposition) to decompose a high rank matrices in to a combination of low rank matrices.\n",
    "rank r can be hyperparameter or manually set , if we set to low rank like 2 , then we will have less parameters to train."
   ]
  },
  {
   "cell_type": "raw",
   "id": "a4cebfb8",
   "metadata": {},
   "source": [
    "QLORA: this is a method which combines both quantization and qlora.\n",
    "In QLoRA, the adapters are low-rank matrices that are trained in full precision.\n",
    "The base model weights are stored in a quantized format to save memory.\n",
    "So we finetune a quantized model using lora technique , the adapter will be in higher precision only inorder to compensate for the quantization errors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2f2d6af",
   "metadata": {},
   "source": [
    "#### Prompt Tuning:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b37aec45",
   "metadata": {},
   "source": [
    "- Dont confuse this with prompt engineering.\n",
    "- This is also a peft technique, here we dont change the architecture of the model.i.e model weights left untouched.\n",
    "- As the model size increases,i.e for larger models this approach achieves as good result as full fine tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fbd4e4b",
   "metadata": {},
   "source": [
    "- Prompt engineering requires lot of manual effort to try different prompts.\n",
    "- In prompt tuning, we add addition trainable parameters/tokens called soft promts to our input prompt and leave it upto supervised learning for determining their optimal value.\n",
    "- By supervised learning, model learns the value for these tokens that maximises performace for given task.\n",
    "- It get preappended to our embeding vecotr that represnets input prompt and it has same lenght as embedding vector.\n",
    "- Somewhere between 20-100 such tokens can be enough for good performance.\n",
    "- Embedding vector of the soft prompts gets updated over time to optimize model completion\n",
    "- Similar to lora, we can train different soft prompts for different tasks and use them during inference.\n",
    "- During inference, we preappend input prompt with trained soft prompt i.e learned tokens , "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "075111e1",
   "metadata": {},
   "source": [
    "# RLHF:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43f49d50",
   "metadata": {},
   "source": [
    "- models  including toxic language in completion,aggressive response,providing dangerous info,offensive response.These happens bcz models are trained with wide variety of data which may contain such data.\n",
    "- models are not honesh in response, even though it doesnt know, it says in confidence. sometimes it doesnt understand context and doesnt give helful answer ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34af4e56",
   "metadata": {},
   "source": [
    "- So Helpful, Honesty, Harmless are important for a model response.\n",
    "- So additional method/fine tuning is required for taking care of these these things in the response so that it should be helful, honest,and harmless and give relevant response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "589d809e",
   "metadata": {},
   "source": [
    "- Popular technique to fine tune a model on human feedback is called RLHF- Reinforcement Learning from Human Feedback. \n",
    "- Instruction Tuned Model--> RLHF--> Human aligned LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc391449",
   "metadata": {},
   "source": [
    "Some advantages or use cases of RLHF:\n",
    "1. Relevant or helpful response\n",
    "2. Harmless response\n",
    "3. Honest response\n",
    "4. Personalization of response or model. For example: personalized ai assistant."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9320d4ee",
   "metadata": {},
   "source": [
    "Lets see what reinforcement learning means in basic first:\n",
    "- Reinforcement learning is a type of machine learning in which an agent learns to make decisions related to a specific goal by taking actions in an environment, with the objective of maximizing some notion of a cumulative reward.\n",
    "- basically like an agent will be given a goal to achieve, so it learns iteratively to make better decision in this process , this will be based on reward system.\n",
    "- In this framework, the agent continually learns from its experiences by taking actions, observing the resulting changes in the environment, and receiving rewards or penalties, based on the outcomes of its actions. By iterating through this process, the agent gradually refines its strategy or policy to make better decisions and increase its chances of success."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30292901",
   "metadata": {},
   "source": [
    "Steps in RLHF:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36845e7c",
   "metadata": {},
   "source": [
    "1. Define model alignment criteria\n",
    "2. Obtain human feedback\n",
    "2. Train a reward model.\n",
    "3. Fine tune the llm with reinforcement learning using the trained reward model."
   ]
  },
  {
   "cell_type": "raw",
   "id": "f0a60b70",
   "metadata": {},
   "source": [
    "Define model alignment criteria: This means, what is the purpose of doing RLHF on our model, is it to reduce toxicity, increase helpfulness, make it more honest? so select the purpose first."
   ]
  },
  {
   "cell_type": "raw",
   "id": "2aadcf5e",
   "metadata": {},
   "source": [
    "Obtain human feedback to train the reward model:\n",
    "Steps:\n",
    "1. Prepare dataset to train the reward model: Take a general instruct llm , give prompts to it and get multiple responses from it. So now we have prompt:completions dataset. \n",
    "2.  we have defined our model alignment criteria initially itself\n",
    "3. Now based on the criteria defined and the prompt:completion dataset, collect the human feedback . \n",
    "Lets see example for this: \n",
    "consider this as a prompt=\"my house it too hot\" \n",
    "completions:\"turn on AC\",\"it is not hot\",\"nothing can be done about heat\"\n",
    "Now the human labellers need to rank them (rank is better than binary selection) based on criteria for example helpfulness. So they will rank turn on ac first becuase that response is much more helpful compared to other responses. it is not hot will be ranked last.\n",
    "Similary for full datset humans feedback will be collected. and for single prompt:completions many humans may do it since we cant rely on one individual.\n",
    "There will be a proper detailed instrutions for human labellers , if they cant decide the rank, they can mark it as fail (skipping). so many such rules will be there.\n",
    "4. Convert rankings into pairwise training data .\n",
    "This means final datset will be like this:\n",
    "\n",
    "prompt                               completions                             reward\n",
    "my house it too hot           turn on AC,it is not hot                       [1,0]\n",
    "                              turn on AC,nothing can be done about heat      [1,0]\n",
    "                              it is not hot,nothing can be done about heat   [0,1]\n",
    "                              "
   ]
  },
  {
   "cell_type": "raw",
   "id": "081b8496",
   "metadata": {},
   "source": [
    "Train the reward model:\n",
    "Now this reward model is also a language model, for example bert, trained using the dataset we prepared in previous step using human labellers.\n",
    "So this model will learn based on the dataset to identify the most helpful response(or what we defined in our criteria) .\n",
    "We can use logits values from this reward model as reward value while fine tuning with RLHF in next step.\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "1abf02a7",
   "metadata": {},
   "source": [
    "Fine tune the llm with reinforcement learning using the trained reward model:\n",
    "Remember LLM which we want to fine tune , should already be instruction fine tuned.\n",
    "\n",
    "Let us now see how RLHF happens:\n",
    "first we pass prompt to llm, it generates completion, we give this prompt:completion pair to reward model, this returns a reward score. now we pass this reward score to Reinforcement anagorithm typically PPO:proximal policy optimization, to update the weights of the llm. more aligned resoponse means higher score,RL algorithm goal is to increase the score. These iterations happens till given epocs as other fine tuning.At the end, the model completion should align and which means higher reward score.We can have stopping criteria based on max reward score as well.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf262f21",
   "metadata": {},
   "source": [
    "Summary of RLHF process:\n",
    "In this process, you make use of a reward model to assess an LLMs completions of a prompt data set against some human preference metric, like helpful or not helpful. Next, you use a reinforcement learning algorithm, in this case, PPO, to update the weights off the LLM based on the reward assigned to the completions generated by the current version off the LLM. You'll carry out this cycle of a multiple iterations using many different prompts and updates off the model weights until you obtain your desired degree of alignment. Your end result is a human aligned LLM that you can use in your application"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5afaf4e2",
   "metadata": {},
   "source": [
    "Potential Problem with this approach: Reward Hacking\n",
    "prompt->instruct llm->detoxicity reward model->RL algo . change weights based on score, so this iteration will keep happening and also for multiple prompts.\n",
    "Now consider promt=this product is\n",
    "completion = complete garbage\n",
    "but we are training to make it detoxify the response, so what i might do inorder to increase score, it may change its response eventually as \"most awesome and world peace all around\".\n",
    "By this process model will return non sensical response inorder to increase score for detoxify. but this is not what we want. we want the response to be in the same context but in a less toxic way may be.\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ea38ee3c",
   "metadata": {},
   "source": [
    "Inorder to avoid Reward Hacking, we can keep initial instruct llm as reference, so we pass prompts to the set of same model, but one model(reference model) we wont change weight i.e weights are frozen, other model will change weight as per RL algo based on score.\n",
    "So the idea is here,\n",
    "During fine tuning, prompts are passed to both models getting response from both, we can compare outputs and calculate kl-divergence, it is a measure of how different two probability distributions are,and from this we will see how much the updated model is diverged from the refernce output.\n",
    "once we caluclate kl-divergence, we also add this term to reward model,so that this will penalize the updated model if it shifts much to the refernece model, and we will get output in the same context but less toxic. \n",
    "every other process is same, just we will have a penalization term in reward model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc848cd5",
   "metadata": {},
   "source": [
    "- For evaluation of rlhf, we can take model before and after rlhf fine tuning and have the pass data to reward model to evaluate the score."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "290171a8",
   "metadata": {},
   "source": [
    "# Buidling LLM based applications"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd68bdc6",
   "metadata": {},
   "source": [
    "From here we will see how we can build llm based applications and different topics which will come along this way."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b06104b2",
   "metadata": {},
   "source": [
    "###  Model Optimization:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be806482",
   "metadata": {},
   "source": [
    "- Now that we have a llm ready, we need to make it ready for inference. but the large models will have higher inference time and takes lot of memory and cant be used for edge devices like may be even mobile apps.\n",
    "- We may need to optimize the llm(reduce size of llm) for faster inference , lower costs, less memory, easy deployment,suitable for edge devices.low latency and other reasons.\n",
    "- So this process will be a tradeoff between accuracy and performace, since optimization will reduce performance, but how much reduction depends on the type of optimization we do."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23b61f13",
   "metadata": {},
   "source": [
    "3 main types of optimization:\n",
    "- Distillation\n",
    "- Quantization\n",
    "- Pruning"
   ]
  },
  {
   "cell_type": "raw",
   "id": "0c1198b7",
   "metadata": {},
   "source": [
    "Distillation:\n",
    "This doesnt reduce size of our llm but it creates new small model\n",
    "knowledge distillation or model distillation is the process of transferring knowledge from a large model to a smaller one\n",
    " \n",
    "In this a larger teacher model will teach a smaller student model, and we use smaller student model for inference.\n",
    "Student model will learn to mimic the teacher model can be in hidden layer or final prediction layer.\n",
    "We will have a training data, we pass this data to both model, take comletion from both the models.\n",
    "Knowledge distillation is achieved by minimizing distillation loss, between probability disribution in softmax layer.\n",
    "This process not effective for generative decoder-only model.\n",
    "We can use it for encoder only models like bert."
   ]
  },
  {
   "cell_type": "raw",
   "id": "ddd44ab0",
   "metadata": {},
   "source": [
    "Quantization:\n",
    "Process of reducing the precision of floating point values i.e. weights of a model to a lower precision value.\n",
    "Here we reduce size of the llm.\n",
    "Once we have a model ready after training/tuning/rlhf, we can do post training quantization before inference to reduce size of model.\n",
    "it transforms model weights into lower precision value, for example into 16 bit or even 8 or 4 bit. but less we go less performance compared to original.but 4bit will make model size very low, which is again our call on tradeoff.\n",
    "Quantization can be applied only to model weights or also activation layer along with weights.\n",
    "Using this we can make use of models on edge devices also.\n",
    "Generally these models will be represeted as Q8 (8 bit int) ,Q4 etc\n",
    "Basically this reduces memory size, cost of fine tuning if we want to do,reduce inference time, and this can reduce performance based on how much precision you are reducing.\n",
    "Example floating point weight matrix : \n",
    "W=\n",
    "0.2    −0.5\n",
    "0.7     1.2\n",
    "\n",
    "now suppose this is float32 values.\n",
    "I want to quantize to Q8 i.e. 8bit integer value. \n",
    "int8 has range of -128 to 127\n",
    "so we will convert each of these weights between that range\n",
    "quantized output matrix:\n",
    "[21,  -53]\n",
    "[74,  127]\n",
    "\n",
    "\n",
    "Mathematical Steps:\n",
    "max = 1.2\n",
    "scaling_factor = 127 / max\n",
    "scaling_factor = 127 / 1.2 ≈ 105.8333\n",
    "Quantize each value (multiply by scaling factor and round to nearest integer)\n",
    "\n",
    "0.2 × 105.8333 ≈ 21.1666 → round to 21\n",
    "-0.5 × 105.8333 ≈ -52.9166 → round to -53\n",
    "0.7 × 105.8333 ≈ 74.0833 → round to 74\n",
    "1.2 × 105.8333 = 127 (exact mapping)\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "759fae43",
   "metadata": {},
   "source": [
    "Pruning:\n",
    "in pruning the goal is to reduce model size for inference by eliminating weights that are not contributing much to overall model performance. removing model weights with value closer or equal to 0.\n",
    "There are different ways for this, it can be during full finetuning, or during peft or post training pruning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5736928e",
   "metadata": {},
   "source": [
    "### RAG:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "100fdbdc",
   "metadata": {},
   "source": [
    "- RAG-Retrieval Augemented Generation\n",
    "- RAG is a framework for building llm prowered systems that make use of external datasources. Be it you want to connect your databse or some documents or connect llm to web,or connecting your proprietary data, we can do all these in RAG. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f992f663",
   "metadata": {},
   "source": [
    "\n",
    "- imagine you have an llm which has a knowdledge cutoff until some point, can we want our llm make use of web to access unknown info rather than generating some stupid things, this will be way.\n",
    "- and also imagine in your company , there are lot of data, which you want to provide to the model, but cant train on it since that is again a tideous process, you can use RAG process so the model just retireves relevant data from datasources and generate content based on that data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9358565d",
   "metadata": {},
   "source": [
    "- Data source can be anything, but generally vector datastores are useful for llm since text data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e83d2985",
   "metadata": {},
   "source": [
    "Example RAG pipeline:\n",
    "user query->query encoder->external data source->combine relevant info with user query->llm->model response\n",
    "\n",
    "- Take the user query\n",
    "- Convert it into embedings\n",
    "- pass this to a vector embedded data source which was vector representation of all our data that we want model to access.We can also chunk the data if it doesnt fit in context window before embedding. we gets symantically related info may be using consine similarity from data source.\n",
    "- take the output from this source, combine with the origin user query/prompt.\n",
    "- Pass this combined prompt to llm and now model will generate a response based on data within the context window.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aca2d973",
   "metadata": {},
   "source": [
    "- LLMs can also solve steps of actions one by one, imagine customer asks for password update, we can call relevant api based on this prompt, get new password from user, pass it to db, update the passord, give response to user. so all these are possible from the llm based applications.\n",
    "- LLMs are generally not good with reasoning or complext tasks, so it is good if we break the task using chain of thought prompting. For example you want to solve a big math problem. give step by solution may be few  shot prompting, then ask it solve the problem, so giving example with intermediate steps will help."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a852c44",
   "metadata": {},
   "source": [
    "- ReAct: Prompting strategy combines chain of thought prompting with action planning, basically to execute a complex task which invloes connecting with external sources, taking step by step actions, connecting with apis etc.\n",
    "- In REACT, prompts are structured in such a way: question, thought, action, observation.\n",
    "- question: problem that requires advanced reasoning and multiple steps to solve\n",
    "- thought:reasoning step on model will tackle the problem and identify actions to take \n",
    "- action: external task that model carries out from set of allowed actions\n",
    "- observation: result of carrying out the action.\n",
    "- Now these steps can happen multiple times to solve a complex problem.\n",
    "- Frameworks like langchain will help in these steps.\n",
    "- Sometimes there can be many paths to take, so to interpret inputs from user and determine what tool to complete the task, this can be done using Agents, in langchain, agents are present for ReAct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4c3d2fb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    }
   ],
   "source": [
    "class CustomFactorial:\n",
    "    def __init__(self, n):\n",
    "        self.n = n\n",
    "\n",
    "    def factorial(self, n=None):\n",
    "        # Use the instance's n if no argument is provided\n",
    "        if n is None:\n",
    "            n = self.n\n",
    "        if n == 0:\n",
    "            return 0\n",
    "        elif n > 0:\n",
    "            if n == 1:\n",
    "                return 1\n",
    "            else:\n",
    "                return n * self.factorial(n - 1)\n",
    "        else:  # n < 0\n",
    "            if n == -1:\n",
    "                return -1\n",
    "            else:\n",
    "                return n * self.factorial(n + 1)\n",
    "\n",
    "# Example usage:\n",
    "cf = CustomFactorial(3)\n",
    "print(cf.factorial())  # Output: -24\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27a473d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomFactorial:\n",
    "    def factorial(self, n):\n",
    "        # Base case for zero\n",
    "        if n == 0:\n",
    "            return 0\n",
    "        # For positive n, standard factorial\n",
    "        elif n > 0:\n",
    "            if n == 1:\n",
    "                return 1\n",
    "            else:\n",
    "                return n * self.factorial(n - 1)\n",
    "        # For negative n, product from n up to -1\n",
    "        else:  # n < 0\n",
    "            if n == -1:\n",
    "                return -1\n",
    "            else:\n",
    "                return n * self.factorial(n + 1)\n",
    "\n",
    "# Example usage:\n",
    "cf = CustomFactorial()\n",
    "print(cf.factorial(5))    # Output: 120\n",
    "print(cf.factorial(0))    # Output: 0\n",
    "print(cf.factorial(-4))   # Output: -24  (i.e., -4 * -3 * -2 * -1)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
